



\section{Articles}
	\subsection {Modeling Documents with a Deep Boltzmann Machine}
		URL: http://www.cs.toronto.edu/~rsalakhu/papers/uai13.pdf \\

		Co-written by Hinton at university of Toronto. Introduce Deep Boltzmann Machine (DBM) for extracting distributed semantic representations. Model is as efficient as Restricted Boltzmann Machine (RBM) 



	\subsection{comparison among Three Neural Networks} 
		https://drive.google.com/drive/\#folders/0ByiAzuPAx4F\_Qzd1bDd2ckpDb2s/0ByiAzuPAx4F\_Z2M2eV81ZEg5VHM/0ByiAzuPAx4F\_ZWJZZ3BIZGFVV2M \\

		Comparing Competitive, Back-propagation (BP) and Radial Basis Function (RBF). 
		\begin{itemize}
			\item intro: state of the art + 3 NN techniques. Definition of positive and negative accuracies.
			\item 2 - presentation of competitive network
			\item 3 - presentation of BP
			\item 4 - Big presentation of RBF. A 3 layer NN with the middle hidden layer. For clustering Euclidean distance is used in between input vectors. 
			\item 5 - Shows efficiency of NN. Emphasize RBF is time efficient.
		\end{itemize}

		\textbf{Dataset}
		Dataset is 20-newsgroup

		\subsubsection{references}
		Feature selection mentioned: Liao S. and Jiang M. “An Improved Method of Feature Selection Based on Concept Attributes in Text Classification”. Lecture Notes in Computer Science, Vol.3610, pp. 1140 – 1149, 2005.

		BP and RBF main article: Mark J. L. Orr. Introduction to Radial Basis Function Networks. Technical report, Center for Cognitive Science, University of Edinburgh, 1996.


	\subsection{Classification using Discriminative Restricted Boltzmann Machines}
		URL: http://machinelearning.org/archive/icml2008/papers/601.pdf \\
		written by Hugo Larochelle and Yoshua Bengio. \\


		\textbf{Dataset}
		Dataset is 20-newsgroup and mnist

	\subsection{Understanding the Difficulty of training deep forward neural networks}
		URL: http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf \\
		written by Xavier Glorot and Yoshua Bengio \\

		The objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural network.
		\begin{itemize}
			\item 1 - state of the art of Deep neural networks
			\item 2 - what are the datasets and their settings
			\item 3 - Discussing the activation functions (sigmoid, tanh and softsign )
			\item 4 - Discussing gradients and their propagation. Normalized initialization of Weights depending on activation function.
			\item 5 - Error curves and conclusion
		\end{itemize}

		\textbf{Dataset}
		MNIST (0 to 9 images), CIFAR-10 (tiny images with horse, cat, ship, truck...), Small-ImageNet (reptiles, vehicles, mammals, instruments...)

		\subsection{Review of Feed Forward Neural Network classification preprocessing techniques }
			http://www.researchgate.net/profile/Roya\_Asadi/publication/263654946\_Review\_of\_feed\_forward\_neural\_network\_classification\_preprocessing\_techniques/links/0046353b74b56bde0b000000\\
			Written in Malaysia\\

			Might be a good summary of existing techniques of pre-processing


		\subsection{Why Does Unsupervised Pre-training Help Deep Learning?}
			URL: http://machinelearning.wustl.edu/mlpapers/paper\_files/AISTATS2010\_ErhanCBV10.pdf \\
			Yoshua Bengio ++ \\


		\subsection{The Difficulty of Training Deep Architectures and the Effect of Unsupervised Pre-Training}
			URL: http://machinelearning.wustl.edu/mlpapers/paper\_files/AISTATS09\_ErhanMBBV.pdf \\
			Yoshua Bengio \\


		\subsection{Exploring Strategies for Training Deep Neural Networks}
			http://machinelearning.wustl.edu/mlpapers/paper\_files/jmlr10\_larochelle09a.pdf \\
